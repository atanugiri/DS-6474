---
title: "HW11"
output: pdf_document
date: "2025-04-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup: Iris Data
We will analyze Fisherâ€™s Iris data using different clustering algorithms. It is a classification problem, as the flowers are grouped into three species. So, we will not use the species variable in the unsupervised learning. However, we will compare the performance of different methods based on the known labels.

## Without Scalling

### Question (a)
Load the data, named iris from base R. Remove the last column Species. Do not center or scale the data.

```{r}
data = iris
data$Species = NULL
head(data)
```

### Question (b)
Perform the K-means clustering algorithm for different values $K$, the number of clusters, say 1 to 5. How many clusters do you recommend? Use this value in the following three questions.
```{r}
k <- 5
WCV <- rep(NA, k)  #within-cluster variation
for (i in 1:k) {
  k_means_out <- kmeans(data, centers = i, nstart = 20)
  WCV[i] <- k_means_out$tot.withinss
}
plot(WCV, xlab = "Number of clusters", type = "b")
```

From the elbow method, k = 3 looks best.

### Question (c)
Use the pair plots and label the clusters. Also, label observations based on the species.
```{r}
# Perform K-means with K = 3
set.seed(123)
k_best <- 3
k_means_out <- kmeans(data, centers = k_best, nstart = 20)
plot(as.data.frame(data), col = k_means_out$cluster, pch = as.numeric(as.factor(iris$Species)))
```

### Question (d)
In high-dimensional data, the pair plots will not be convenient. So, use one or two plots using the principal components.
```{r}
pca_out <- prcomp(data)
PC1 <- pca_out$x[, 1]
PC2 <- pca_out$x[, 2]
plot(PC1, PC2, col = k_means_out$cluster, pch = as.numeric(as.factor(iris$Species)),
  main = "K-means Clustering", xlab = "First Principal Component", ylab = "Second Principal Component")

legend("topright", legend = levels(as.factor(iris$Species)), pch = 1:3)
legend("bottom", legend = c("Cluster 1", "Cluster 2", "Cluster 3"), col = 1:3, lty = 1)
```

### Question (e)
Calculate the confusion matrix based on the species variable. Also, compute a measure of accuracy. Note that the clusters may come in any order. So you need to consider permuting the rows of the confusion matrix to find the accuracy of clustering.
```{r}
Best_Confusion_Matrix <- function(Confusion_Matrix) {
  # Computes the best confusion matrix by permuting the rows
  library(combinat)
  x <- as.matrix(Confusion_Matrix)
  n <- sum(x)  # sample size
  p <- nrow(x)  # number of features
  if (ncol(x) != p)
    stop("x must be a square matrix.")

  perm_vec <- permn(1:p)  # all permutations of 1:p
  p_fact <- length(perm_vec)  # p factorial
  accuracy_val <- rep(NA, p_fact)

  # checking
  for (i in 1:p_fact) accuracy_val[i] <- sum(diag(x[perm_vec[[i]], ]))/n

  best_accuracy_index <- which.max(accuracy_val)
  best_accuracy_val <- accuracy_val[best_accuracy_index]
  best_Conf_Mat <- x[perm_vec[[best_accuracy_index]], ]

  return(list(x = best_Conf_Mat, val = best_accuracy_val))
}
```

```{r}
confusioin_mat <- table(k_means_out$cluster, iris$Species)
Best_Confusion_Matrix(confusioin_mat)
```

### Question (f)
Use hierarchical clustering with complete, average, and single linkages. Select the best cluster from each of these methods. Visualize the clusters using the first two principal components (as in d) and find the accuracy measures (as in e).
```{r}
cluster_complete <- hclust(dist(data), method = "complete")
cluster_average <- hclust(dist(data), method = "average")
cluster_single <- hclust(dist(data), method = "single")

par(mfrow = c(1, 3))
plot(cluster_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.9)
plot(cluster_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.9)
plot(cluster_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.9)
```

The complete, average, and single linkages suggest six, four, and two clusters, respectively. However, we will consider three clusters for all cases for simplicity and ease of comparison.

\textbf{Complete linkage}
```{r}
cluster_complete_3 = cutree(cluster_complete, k = 3)
cc_table = table(cluster_complete_3, iris$Species)
Best_Confusion_Matrix(cc_table)

plot(PC1, PC2, col = cluster_complete_3, pch = as.numeric(as.factor(iris$Species)))
```

\textbf{Average linkage}
```{r}
cluster_average_3 = cutree(cluster_average, k = 3)
ca_table = table(cluster_average_3, iris$Species)
Best_Confusion_Matrix(ca_table)

plot(PC1, PC2, col = cluster_average_3, pch = as.numeric(as.factor(iris$Species)))
```

\textbf{Single linkage}
```{r}
cluster_single_3 = cutree(cluster_single, k = 3)
cs_table = table(cluster_single_3, iris$Species)
Best_Confusion_Matrix(cs_table)

plot(PC1, PC2, col = cluster_single_3, pch = as.numeric(as.factor(iris$Species)))
```


## With Scalling
### Question (a)
Answer the above questions using the scaled data (each variable will have a standard deviation of one). You may skip the scatter plot of question c.

```{r}
data = scale(data)
head(data)
```

### Question (b)
Perform the K-means clustering algorithm for different values $K$, the number of clusters, say 1 to 5. How many clusters do you recommend? Use this value in the following three questions.
```{r}
set.seed(123)
wcv = numeric(5)
for (i in 1:5) {
  k_means_out = kmeans(data, centers = i, nstart = 20)
  wcv[i] = k_means_out$tot.withinss
}
plot(wcv, type = "b", main = 'Scaled data')
```

In the scree plot, the within-cluster variation (WCV) are almost flat after $K=3$. So, we plan to consider three clusters using the elbow method.
```{r}
k_best = 3
k_means_out = kmeans(data, centers = k_best, nstart = 20)
k_means_out
```

### Question (c)
Use the pair plots and label the clusters. Also, label observations based on the species.
```{r}
pairs(data, col = k_means_out$cluster, pch = as.numeric(iris$Species))
```

### Question (d)
In high-dimensional data, the pair plots will not be convenient. So, use one or two plots using the principal components.
```{r}
pc_out = prcomp(data)
PC1 = pc_out$x[,1]
PC2 = pc_out$x[,2]
plot(PC1, PC2, col = k_means_out$cluster, pch = as.numeric(iris$Species))
legend("topright", legend = levels(as.factor(iris$Species)), pch = 1:3)
legend("bottom", legend = c("Cluster 1", "Cluster 2", "Cluster 3"), col = 1:3, lty = 1)
```

<!-- h. -->
<!-- ```{r} -->
<!-- # PCA on scaled data -->
<!-- pca_scaled <- prcomp(iris_scaled) -->

<!-- # Variance explained by each PC -->
<!-- var_explained <- pca_scaled$sdev^2 / sum(pca_scaled$sdev^2) -->

<!-- # Cumulative variance explained -->
<!-- cum_var_explained <- cumsum(var_explained) -->

<!-- # Plot -->
<!-- plot(cum_var_explained, xlab = "Number of Principal Components", -->
<!--      ylab = "Cumulative Proportion of Variance Explained", -->
<!--      type = "b", pch = 19, -->
<!--      main = "Cumulative Variance Explained by PCs") -->
<!-- abline(h = 0.9, col = "red", lty = 2)  # optional: 90% variance line -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Approximated data using first 2 PCs -->
<!-- approx_data <- pca_scaled$x[, 1:2] -->
<!-- ``` -->

<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- wss_approx <- numeric(5) -->

<!-- for (k in 1:5) { -->
<!--   kmeans_approx <- kmeans(approx_data, centers = k, nstart = 25) -->
<!--   wss_approx[k] <- kmeans_approx$tot.withinss -->
<!-- } -->

<!-- # Elbow Plot -->
<!-- plot(1:5, wss_approx, type = "b", pch = 19, -->
<!--      xlab = "Number of Clusters K (Approximated Data)", -->
<!--      ylab = "Total WSS", -->
<!--      main = "Elbow Method (Approximated Data)") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # K-means with K=3 -->
<!-- set.seed(123) -->
<!-- kmeans_approx_final <- kmeans(approx_data, centers = 3, nstart = 25) -->

<!-- # Confusion matrix -->
<!-- table_kmeans_approx <- table(kmeans_approx_final$cluster, iris$Species) -->

<!-- # Best matching confusion matrix and accuracy -->
<!-- result_kmeans_approx <- Best_Confusion_Matrix(table_kmeans_approx) -->

<!-- cat("K-means (approximated) Best Confusion Matrix:\n") -->
<!-- print(result_kmeans_approx$x) -->
<!-- cat(sprintf("Accuracy: %.4f\n\n", result_kmeans_approx$val)) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- # Distance matrix -->
<!-- dist_approx <- dist(approx_data) -->

<!-- # Hierarchical clustering -->
<!-- hc_complete_approx <- hclust(dist_approx, method = "complete") -->
<!-- hc_average_approx  <- hclust(dist_approx, method = "average") -->
<!-- hc_single_approx   <- hclust(dist_approx, method = "single") -->

<!-- # Cut into 3 clusters -->
<!-- cluster_complete_approx <- cutree(hc_complete_approx, k = 3) -->
<!-- cluster_average_approx  <- cutree(hc_average_approx, k = 3) -->
<!-- cluster_single_approx   <- cutree(hc_single_approx, k = 3) -->

<!-- # Confusion matrices -->
<!-- table_complete_approx <- table(cluster_complete_approx, iris$Species) -->
<!-- table_average_approx  <- table(cluster_average_approx, iris$Species) -->
<!-- table_single_approx   <- table(cluster_single_approx, iris$Species) -->

<!-- # Accuracy calculations -->
<!-- result_complete_approx <- Best_Confusion_Matrix(table_complete_approx) -->
<!-- result_average_approx  <- Best_Confusion_Matrix(table_average_approx) -->
<!-- result_single_approx   <- Best_Confusion_Matrix(table_single_approx) -->

<!-- # Print results -->
<!-- cat("Complete Linkage (approximated) Best Confusion Matrix:\n") -->
<!-- print(result_complete_approx$x) -->
<!-- cat(sprintf("Accuracy: %.4f\n\n", result_complete_approx$val)) -->

<!-- cat("Average Linkage (approximated) Best Confusion Matrix:\n") -->
<!-- print(result_average_approx$x) -->
<!-- cat(sprintf("Accuracy: %.4f\n\n", result_average_approx$val)) -->

<!-- cat("Single Linkage (approximated) Best Confusion Matrix:\n") -->
<!-- print(result_single_approx$x) -->
<!-- cat(sprintf("Accuracy: %.4f\n", result_single_approx$val)) -->
<!-- ``` -->

<!-- We got very good clustering with 2 PCs. -->